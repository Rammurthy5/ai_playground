Masked Language Model are commonly used for non-generative tasks like finding the missing token using the context from preceding and following tokens, sentiment analysis, text classification. BERT is a MLM.

they're also useful in understanding overall context. for e.g. code debugging where it needs to understand preceding and following code to identify errors.
